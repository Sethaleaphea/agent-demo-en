# 知识库构建

首先，使用minerU的OCR模式对所有pdf进行解析，对于minerU的解析结果，用人工定义的规则进行进一步清洗和纠错。最终，每个pdf对应一个txt文件。所有的txt解析结果都在目录/home/dalhxwlyjsuo/guest/result 下。

之后，将txt转成json格式，方便后续构建知识库：

```
def folder_to_jsonl(folder_path, output_file):
    """
    Converts all TXT files in a folder into JSONL format, each file's content as a single entry.

    Args:
        folder_path (str): Path to the folder containing TXT files.
        output_file (str): Path to the output JSONL file.
    """
    jsonl_entries = []
    file_id = 0

    # Iterate through all files in the folder
    for filename in os.listdir(folder_path):
        if filename.endswith(".txt"):
            file_path = os.path.join(folder_path, filename)
            
            # Read the content of the TXT file
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Clean up the content
            content = " ".join(content.split())

            # Create a JSONL entry
            jsonl_entries.append({
                "id": file_id,
                "contents": content
            })
            file_id += 1

    # Write all entries to the JSONL output file
    with open(output_file, 'w', encoding='utf-8') as f:
        for entry in jsonl_entries:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')

    print(f"Processed {file_id} TXT files. Output saved to {output_file}")
    
# Usage
if __name__ == "__main__":
    folder_path = './txtResult'  # Replace with the path to your folder
    output_path = './corpust.jsonl'     # Replace with the desired output file path
    folder_to_jsonl(folder_path, output_path)
```

上面的代码的作用是遍历一个文件夹中的所有txt文件，并将其转成一个jsonl文件，其中，json文件的每行为一个txt的全部内容。接下来，使用chonkie库，对文本进行分块：

```
import argparse
import json
from tqdm import tqdm
import chonkie


def load_jsonl(file_path):
    """
    Load JSONL file into a list of dictionaries.
    """
    documents = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            documents.append(json.loads(line))
    return documents


def save_jsonl(documents, file_path):
    """
    Save a list of dictionaries into a JSONL file.
    """
    with open(file_path, 'w', encoding='utf-8') as f:
        for doc in documents:
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')  # ensure_ascii=False 避免乱码


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Chunk documents from a JSONL file.")
    parser.add_argument("--input_path", type=str, required=True, help="Path to input JSONL file")
    parser.add_argument("--output_path", type=str, required=True, help="Path to output JSONL file")
    parser.add_argument("--chunk_by", default="token", choices=["token", "word", "sentence", "recursive"],
                        help="Chunking method to use")
    parser.add_argument("--chunk_size", default=512, type=int, help="Size of chunks")
    parser.add_argument("--tokenizer_name_or_path", default='o200k_base', type=str)

    args = parser.parse_args()

    # Load documents
    print("Loading documents...")
    documents = load_jsonl(args.input_path)

    # Initialize chunker
    if args.chunk_by == "token":
        chunker = chonkie.TokenChunker(tokenizer=args.tokenizer_name_or_path, chunk_size=args.chunk_size)
    elif args.chunk_by == "sentence":
        chunker = chonkie.SentenceChunker(tokenizer=args.tokenizer_name_or_path, chunk_size=args.chunk_size)
    elif args.chunk_by == "recursive":
        chunker = chonkie.RecursiveChunker(tokenizer=args.tokenizer_name_or_path, chunk_size=args.chunk_size, min_characters_per_chunk=1)
    elif args.chunk_by == "word":
        chunker = chonkie.WordChunker(tokenizer=args.tokenizer_name_or_path, chunk_size=args.chunk_size)
    else:
        raise ValueError(f"Invalid chunking method: {args.chunk_by}")

    # Process and chunk documents
    print("Chunking documents...")
    chunked_documents = []
    current_chunk_id = 0
    for doc in tqdm(documents):
        # Extract 'contents' from document
        text = doc['contents']  # 确保只处理'contents'字段
        chunks = chunker.chunk(text)
        for chunk in chunks:
            chunked_doc = {
                'id': current_chunk_id,
                'doc_id': doc['id'],  # 保留原始文档的 ID
                'contents': chunk.text,  # 仅保留 chunk 的内容
            }
            chunked_documents.append(chunked_doc)
            current_chunk_id += 1

    # Save chunked documents
    print("Saving chunked documents...")
    save_jsonl(chunked_documents, args.output_path)
    print(f"Done! Processed {len(documents)} documents into {len(chunked_documents)} chunks.")

```

上面的代码每次可以处理一个jsonl文件，并将其按照参数的设置，切成若干chunk，并保存为jsonl文件。将jsonl的每条记录切分为chunk后，便可以使用预训练好的embedding模型建立faiss向量数据库索引：

```
import os
import json
import pickle
import pandas as pd
from tqdm import tqdm

from trustrag.modules.document.chunk import TextChunker
from trustrag.modules.document.txt_parser import TextParser
from trustrag.modules.document.utils import PROJECT_BASE
from trustrag.modules.generator.llm import GLM4Chat
from trustrag.modules.reranker.bge_reranker import BgeRerankerConfig, BgeReranker
from trustrag.modules.retrieval.bm25s_retriever import BM25RetrieverConfig
from trustrag.modules.retrieval.dense_retriever import DenseRetrieverConfig,DenseRetriever
from trustrag.modules.retrieval.hybrid_retriever import HybridRetriever, HybridRetrieverConfig

embedding_model_path = "./models/bge-large-zh-v1.5"
def load_json_from_folder(folder_path):
    """
    Load JSON objects from all files in a folder into a single list.
    
    Args:
        folder_path (str): Path to the folder containing JSON files.
    
    Returns:
        list: A list of JSON objects.
    """
    documents = []
    
   # 遍历文件夹中的所有文件
    for file_name in os.listdir(folder_path):
        if file_name.endswith('.jsonl'):  # 检查是否为 JSONL 文件
            file_path = os.path.join(folder_path, file_name)
            
            # 打开 JSON 文件并读取每一行
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    data = json.loads(line.strip())  # 加载 JSON 数据
                    contents = data.get('contents', None)  # 提取 `contents` 键的内容
                    if isinstance(contents, str):  # 确保内容是字符串
                        documents.append(contents)  # 添加到 documents 列表中
    
    return documents

# 使用示例
folder_path = './chunked_jsonl' 
documents = load_json_from_folder(folder_path)
print(documents)
dense_config = DenseRetrieverConfig(
    model_name_or_path=embedding_model_path,
    dim=1024,
    index_path='indexs/dense_cache'
)
config_info = dense_config.log_config()

dense_retriever = DenseRetriever(config=dense_config)


# Build the index
dense_retriever.build_from_texts(documents)
# Save the index
dense_retriever.save_index()
```

上面的代码作用为遍历一个文件夹下的所有jsonl文件，并将其中的所有chunk存储到faiss中，并建立索引。最后，下面的代码使用trustRAG项目实现的检索器，对faiss向量数据库进行检索，测试代码如下：

```
import os
import json
import pickle
import pandas as pd
from tqdm import tqdm

from trustrag.modules.document.chunk import TextChunker
from trustrag.modules.document.txt_parser import TextParser
from trustrag.modules.document.utils import PROJECT_BASE
from trustrag.modules.generator.llm import GLM4Chat
from trustrag.modules.reranker.bge_reranker import BgeRerankerConfig, BgeReranker
from trustrag.modules.retrieval.bm25s_retriever import BM25RetrieverConfig
from trustrag.modules.retrieval.dense_retriever import DenseRetrieverConfig,DenseRetriever
from trustrag.modules.retrieval.hybrid_retriever import HybridRetriever, HybridRetrieverConfig

embedding_model_path = "./models/bge-large-zh-v1.5"
dense_config = DenseRetrieverConfig(
    model_name_or_path=embedding_model_path,
    dim=1024,
    index_path='indexs/dense_cache'
)
config_info = dense_config.log_config()

dense_retriever = DenseRetriever(config=dense_config)

dense_retriever.load_index()
query = "如何进行班级管理"
results = dense_retriever.retrieve(query, top_k=5)
print(len(results))
# Output results
for result in results:
    print(f"Text: {result['text']}, Score: {result['score']}")
```

