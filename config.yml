data:
  input_dir: D:\code\agent_demo\kg\in
  output_dir: D:\code\agent_demo\kg\out
  ontology_dir: D:\code\agent_demo\kg\ontology
openai:
  api_key: "sk-EQY6oy2D9Brb2IWxymHTnmeFz5unIyobzHtjDgFs2ZwEPjmY"
  temperature: 0.7  # optional, defaults to 0.7
  max_tokens: 4000  # optional, defaults to 2000
  api_base: "https://dashscope.aliyuncs.com/compatible-mode/v1"
  timeout: 60  # optional, defaults to 60
  max_retries: 3  # optional, defaults to 3
ollama:
  api_base: "http://localhost:11434"
  timeout: 60  # optional, defaults to 60
  max_retries: 3  # optional, defaults to 3
  temperature: 0.7  # optional, defaults to 0.7
  context_window: 4096  # optional, defaults to 5
  format: "json"  # optional, defaults to "json"
  stream: false  # optional, defaults to false
huggingface:
  api_token: "YOUR_API_TOKEN"
  api_base: "https://api-inference.huggingface.co"
  timeout: 60  # optional, defaults to 60
llamacpp:
  context_length: 2048  # Maximum context length
  num_threads: 4        # Number of CPU threads to use
  gpu_layers: 0        # Number of layers to offload to GPU (0 for CPU-only)
  max_tokens: 100      # Maximum number of tokens to generate
  temperature: 0.7     # Temperature for sampling
  top_p: 0.9          # Top-p sampling parameter
  stop_tokens: [ "\n" ]  # Tokens that will stop generation
  model_path: "YOUR_MODEL_PATH"
semantic_kg:
  entity_list: D:\code\agent_demo\kg\ontology\entity.csv
  relation_list: D:\code\agent_demo\kg\ontology\relatiom.csv
  ontology: D:\code\agent_demo\kg\ontology\ontology.json
  domain_description: D:\code\agent_demo\kg\ontology\domain_description.txt